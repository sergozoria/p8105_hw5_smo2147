---
title: "Homework 5 - P8105"
author: "Sergio Ozoria Ramírez (smo2147)"
date: "2025-11-13"
output: github_document
---

# Set up

Potential packages are first loaded to use for our problem sets. Figure preferences were used to set figure sizing and aesthetics for plotting outputs. 

```{r set up, message = FALSE}
library(tidyverse)
library(modelr)
library(rvest)
library(scales)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

Problem set 1 outlines the function to be used to estimate the probability of sharing a birthday in a calendar year for a group size of 2:50. A figure was plotted to observe these estimates using `ggplot`. 

```{r bday function}
bday_prob = function(n) {
  
  if (!is.numeric(n) || length(n) != 1 || n < 1) {
    
    stop("n must be a single positive number") 
    
    }

  bday_samp = sample(1:365, size = n, replace = TRUE)
  
  any(duplicated(bday_samp))
  
}
```

```{r bday simulation}
set.seed(123)

bday_results =
  tibble(
    n = 2:50,
    bday_probability = map_dbl(
      n, \(x) mean(replicate(10000, bday_prob(x)
                             )
                   )
      )
  )

bday_results
```

```{r figure 1}
bday_results |> 
  ggplot(aes(x = n, y = bday_probability)) +
  geom_hline(yintercept = 0.5, linetype = "dashed") +
  geom_point(color = "blue") +
  scale_y_continuous(labels = percent_format()) +
  labs(
    title = "Figure 1. Probability of Shared Birthday by Group Size",
    x = "Group size (n)",
    y = "Probability of shared birthday"
  )
```

#### Figure 1 - Comments

Figure 1 shows that the probability of sharing a birthday increases as the group size increases. For example, in a random group of 23 people, there is a 50% chance that at least two people share the same birthday. We see that the probability of shared birthdays among two people increases closer to 100% with at least 40 people in a random sample. This is because we're making comparisons based on matches, so the rate of possible shared birthdays increases at a faster rate than the group size

# Problem 2

```{r mu & pval function}
sim_test = function(mu, n = 30, sigma = 5) {
  
  x = rnorm(n, mean = mu, sd = sigma)
  
  t_test = t.test(x, mu = 0)
  
  broom::tidy(t_test) |> 
    select(estimate, p.value)
  
}
```


```{r}
sim_mu0 = 
  tibble(
    iter = 1:5000) |> 
  mutate(
    tt_results = map(iter, \(i) sim_test(mu = 0))
  ) |> 
  unnest(tt_results)

sim_mu0
```


```{r}
sim_mu_06 =
  expand_grid(
    mu = 0:6,
    iter = 1:5000
  ) |> 
  mutate(
    tt_results = map(mu, sim_test)
  ) |> 
    unnest(tt_results)

sim_mu_06
```


```{r figure 2}
power_df =
  sim_mu_06 |> 
  group_by(mu) |> 
  summarize(power = mean(p.value < 0.05))

power_df |> 
  ggplot(aes(x = mu, y = power)) +
  geom_line(color = "skyblue") +
  geom_point(color = "blue", size = 2) +
  labs(
    title = "Figure 2. Association between Effect Size and Power",
    x = "True population mean",
    y = "Power (Prob. of pejection null)"
  )
```

#### Figure 2 - Comments

Figure 2 shows that the proportion of rejecting the null, meaning power, increases as the true population mean moves further away from 0. Notably, when the true population mean increases from 2 to 3, the proportion of rejecting the null increases significantly, closer to a power of 1

```{r summary table for average estimates}
avg_est_summ =
  sim_mu_06 |> 
  group_by(mu) |> 
  summarize(
    avg_est_all = mean(estimate),
    avg_est_reject = mean(estimate[p.value < .05])
  )

avg_est_summ |> 
  rename(
    `True µ` = mu,
    `Average µˆ (All)` = avg_est_all,
    `Average µˆ (Rejected)` = avg_est_reject
  ) |> 
  knitr::kable(digits = 3)
```


```{r figure 3}
avg_est_summ |> 
  ggplot(aes(x = mu)) +
  geom_line(aes(y = avg_est_all), color = "blue") +
  geom_point(aes(y = avg_est_all), color = "blue") +
  geom_line(aes(y = avg_est_reject), color = "red") +
  geom_point(aes(y = avg_est_reject), color = "red") +
  labs(
    title = "Figure 3. Average Sample Estimates vs True Population Estimates",
    x = "True population (µ)",
    y = "Average sample estimate (µˆ)"
  )
```

#### Figure 3 - Comments

Figure 3 shows the average estimates of µˆ across 5000 samples, along with the estimates of statistically significant samples with p-value of less than 0.05 in which the null hypothesis was rejected. From these results, we can infer that the average estimates of µˆ among the rejected tests is not equal to the true value of the true population mean, particularly when the true mean is small. This happens because when the true mean is close to 0, the test only rejects the null in samples where the observed value is larger than what it's expected. Based on the plotted estimates, we can conclude that as the true mean increases, more samples will exceed the rejection threshold, making the average estimate among rejected test become closer to the true population mean

# Problem 3

```{r loading US homicide data}
wash_homi_df =
  read.csv("./Data/homicide-data.csv", na = c("NA", ".", "")) |> 
  janitor::clean_names()
```

#### Description of Washington Post Homicide Dataset

This Washington Post homicide dataset contains **`r nrow(wash_homi_df)`** observations and **`r ncol(wash_homi_df)`** variables that provide national data on criminal homicides across 50 of the largest American cities over the past decade, including key variables such as data of criminal act, victims' basic demographic data such as sex, age, and race/ethnicity, the city and state in which the homicide occurred, and the status of homicide investigation (i.e., `disposition`)

#### Total Number of Homicides & Unsolved Homicides by City and Status of Investigation

Let's first create a new variable integrating cities with their corresponding state, followed by a summary of the total number of homicides and the number of unsolved homicides by status of investigation

```{r homicide summary}
wash_homi_df = 
  wash_homi_df |>
  mutate(
    city_state = str_c(city, state, sep = ", ")
  ) 

homi_summ =
  wash_homi_df |> 
  group_by(city_state) |> 
  summarize(
    t_homi = n(),
    t_unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

homi_summ
```


```{r estimating proportion of homicides}
filter_balt =
  wash_homi_df |> 
  filter(city_state == "Baltimore, MD")

count_balt =
  filter_balt |> 
  summarize(
    total = n(),
    unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

test_balt =
  prop.test(
    x = count_balt$unsolved,
    n = count_balt$total
  ) |> 
  broom::tidy()

test_balt |> select(estimate, conf.low, conf.high) |> 
  knitr::kable(digits = 3)
```

Based on these estimates, we can conclude that about 64.6% of total homicides committed in the city of Baltimore are not solved. Therefore, we are 95% confident that the true proportion of unsolved homicides in the city of Baltimore, however, lies between 62.8% and 66.3%

```{r running prop.test for each city}
city_ptest =
  wash_homi_df |> 
  group_by(city_state) |> 
  summarize(
    total = n(),
    unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    .groups = "drop"
  ) |> 
  mutate(
    c_ptest = map2(unsolved, total, \(u, t) prop.test(u, t)),
    tidy_c_ptest = map(c_ptest, broom::tidy)
  ) |> 
  unnest(tidy_c_ptest) |> 
  select(
    city_state,
    estimate,
    conf.low,
    conf.high
  )

city_ptest
```


```{r fig.width = 10, fig.height = 7}
city_ptest |> 
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) |> 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_errorbar(
    aes(ymin = conf.low, ymax = conf.high)
  ) +
  geom_point(color = "blue", size = 2) +
  coord_flip() +
  labs(
    title = "Figure 4. Proportion of Unsolved Homicides by Cities in the United States",
    x = "Cities",
    y = "Estimates (95% CI)"
  )
  
```

#### Figure 4 - Comments

Figure 4 shows the proportion of unsolved homicides sorted by cities from lowest to highest, showing their estimated proportions along with their 95 confidence interval range. From this figure, the city of Chicago has the highest proportion of unsolved homicides at around 73%, where the city of Tulsa either reports a proportion of 0% unsolved or there's no data on the proportion of crimes unsolved. Overall, the city of Richmond reports one of the lowest proportion of unsolved homicides among our sample population.

Homework 5 - P8105
================
Sergio Ozoria Ramírez (smo2147)
2025-11-13

# Set up

Potential packages are first loaded to use for our problem sets. Figure
preferences were used to set figure sizing and aesthetics for plotting
outputs.

``` r
library(tidyverse)
library(modelr)
library(rvest)
library(scales)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

Problem set 1 outlines the function to be used to estimate the
probability of sharing a birthday in a calendar year for a group size of
2:50. A figure was plotted to observe these estimates using `ggplot`.

``` r
bday_prob = function(n) {
  
  if (!is.numeric(n) || length(n) != 1 || n < 1) {
    
    stop("n must be a single positive number") 
    
    }

  bday_samp = sample(1:365, size = n, replace = TRUE)
  
  any(duplicated(bday_samp))
  
}
```

``` r
set.seed(123)

bday_results =
  tibble(
    n = 2:50,
    bday_probability = map_dbl(
      n, \(x) mean(replicate(10000, bday_prob(x)
                             )
                   )
      )
  )

bday_results
```

    ## # A tibble: 49 × 2
    ##        n bday_probability
    ##    <int>            <dbl>
    ##  1     2           0.0033
    ##  2     3           0.0079
    ##  3     4           0.0158
    ##  4     5           0.0248
    ##  5     6           0.0387
    ##  6     7           0.057 
    ##  7     8           0.0749
    ##  8     9           0.0885
    ##  9    10           0.116 
    ## 10    11           0.140 
    ## # ℹ 39 more rows

``` r
bday_results |> 
  ggplot(aes(x = n, y = bday_probability)) +
  geom_hline(yintercept = 0.5, linetype = "dashed") +
  geom_point(color = "blue") +
  scale_y_continuous(labels = percent_format()) +
  labs(
    title = "Figure 1. Probability of Shared Birthday by Group Size",
    x = "Group size (n)",
    y = "Probability of shared birthday"
  )
```

<img src="p8105_hw5_smo2147_files/figure-gfm/figure 1-1.png" width="90%" />

#### Figure 1 - Comments

Figure 1 shows that the probability of sharing a birthday increases as
the group size increases. For example, in a random group of 23 people,
there is a 50% chance that at least two people share the same birthday.
We see that the probability of shared birthdays among two people
increases closer to 100% with at least 40 people in a random sample.
This is because we’re making comparisons based on matches, so the rate
of possible shared birthdays increases at a faster rate than the group
size

# Problem 2

``` r
sim_test = function(mu, n = 30, sigma = 5) {
  
  x = rnorm(n, mean = mu, sd = sigma)
  
  t_test = t.test(x, mu = 0)
  
  broom::tidy(t_test) |> 
    select(estimate, p.value)
  
}
```

``` r
sim_mu0 = 
  tibble(
    iter = 1:5000) |> 
  mutate(
    tt_results = map(iter, \(i) sim_test(mu = 0))
  ) |> 
  unnest(tt_results)

sim_mu0
```

    ## # A tibble: 5,000 × 3
    ##     iter estimate p.value
    ##    <int>    <dbl>   <dbl>
    ##  1     1   -0.424  0.642 
    ##  2     2    1.34   0.220 
    ##  3     3   -1.85   0.0246
    ##  4     4   -0.759  0.373 
    ##  5     5   -1.03   0.259 
    ##  6     6    0.502  0.521 
    ##  7     7    0.442  0.573 
    ##  8     8   -0.539  0.593 
    ##  9     9   -0.293  0.739 
    ## 10    10    0.955  0.271 
    ## # ℹ 4,990 more rows

``` r
sim_mu_06 =
  expand_grid(
    mu = 0:6,
    iter = 1:5000
  ) |> 
  mutate(
    tt_results = map(mu, sim_test)
  ) |> 
    unnest(tt_results)

sim_mu_06
```

    ## # A tibble: 35,000 × 4
    ##       mu  iter estimate p.value
    ##    <int> <int>    <dbl>   <dbl>
    ##  1     0     1  -0.536    0.517
    ##  2     0     2  -0.678    0.431
    ##  3     0     3   0.784    0.377
    ##  4     0     4  -1.02     0.269
    ##  5     0     5   0.0464   0.953
    ##  6     0     6  -1.46     0.104
    ##  7     0     7  -0.0818   0.930
    ##  8     0     8   0.910    0.258
    ##  9     0     9   0.300    0.750
    ## 10     0    10  -0.492    0.517
    ## # ℹ 34,990 more rows

``` r
power_df =
  sim_mu_06 |> 
  group_by(mu) |> 
  summarize(power = mean(p.value < 0.05))

power_df |> 
  ggplot(aes(x = mu, y = power)) +
  geom_line(color = "skyblue") +
  geom_point(color = "blue", size = 2) +
  labs(
    title = "Figure 2. Association between Effect Size and Power",
    x = "True population mean",
    y = "Power (Prob. of pejection null)"
  )
```

<img src="p8105_hw5_smo2147_files/figure-gfm/figure 2-1.png" width="90%" />

#### Figure 2 - Comments

Figure 2 shows that the proportion of rejecting the null, meaning power,
increases as the true population mean moves further away from 0.
Notably, when the true population mean increases from 2 to 3, the
proportion of rejecting the null increases significantly, closer to a
power of 1

``` r
avg_est_summ =
  sim_mu_06 |> 
  group_by(mu) |> 
  summarize(
    avg_est_all = mean(estimate),
    avg_est_reject = mean(estimate[p.value < .05])
  )

avg_est_summ |> 
  rename(
    `True µ` = mu,
    `Average µˆ (All)` = avg_est_all,
    `Average µˆ (Rejected)` = avg_est_reject
  ) |> 
  knitr::kable(digits = 3)
```

| True µ | Average µˆ (All) | Average µˆ (Rejected) |
|-------:|-----------------:|----------------------:|
|      0 |           -0.012 |                 0.044 |
|      1 |            0.992 |                 2.275 |
|      2 |            1.997 |                 2.599 |
|      3 |            2.993 |                 3.184 |
|      4 |            3.988 |                 4.021 |
|      5 |            4.995 |                 4.996 |
|      6 |            5.988 |                 5.988 |

``` r
avg_est_summ |> 
  ggplot(aes(x = mu)) +
  geom_line(aes(y = avg_est_all), color = "blue") +
  geom_point(aes(y = avg_est_all), color = "blue") +
  geom_line(aes(y = avg_est_reject), color = "red") +
  geom_point(aes(y = avg_est_reject), color = "red") +
  labs(
    title = "Figure 3. Average Sample Estimates vs True Population Estimates",
    x = "True population (µ)",
    y = "Average sample estimate (µˆ)"
  )
```

<img src="p8105_hw5_smo2147_files/figure-gfm/figure 3-1.png" width="90%" />

#### Figure 3 - Comments

Figure 3 shows the average estimates of µˆ across 5000 samples, along
with the estimates of statistically significant samples with p-value of
less than 0.05 in which the null hypothesis was rejected. From these
results, we can infer that the average estimates of µˆ among the
rejected tests is not equal to the true value of the true population
mean, particularly when the true mean is small. This happens because
when the true mean is close to 0, the test only rejects the null in
samples where the observed value is larger than what it’s expected.
Based on the plotted estimates, we can conclude that as the true mean
increases, more samples will exceed the rejection threshold, making the
average estimate among rejected test become closer to the true
population mean

# Problem 3

``` r
wash_homi_df =
  read.csv("./Data/homicide-data.csv", na = c("NA", ".", "")) |> 
  janitor::clean_names()
```

#### Description of Washington Post Homicide Dataset

This Washington Post homicide dataset contains **52179** observations
and **12** variables that provide national data on criminal homicides
across 50 of the largest American cities over the past decade, including
key variables such as data of criminal act, victims’ basic demographic
data such as sex, age, and race/ethnicity, the city and state in which
the homicide occurred, and the status of homicide investigation (i.e.,
`disposition`)

#### Total Number of Homicides & Unsolved Homicides by City and Status of Investigation

Let’s first create a new variable integrating cities with their
corresponding state, followed by a summary of the total number of
homicides and the number of unsolved homicides by status of
investigation

``` r
wash_homi_df = 
  wash_homi_df |>
  mutate(
    city_state = str_c(city, state, sep = ", ")
  ) 

homi_summ =
  wash_homi_df |> 
  group_by(city_state) |> 
  summarize(
    t_homi = n(),
    t_unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

homi_summ
```

    ## # A tibble: 51 × 3
    ##    city_state      t_homi t_unsolved
    ##    <chr>            <int>      <int>
    ##  1 Albuquerque, NM    378        146
    ##  2 Atlanta, GA        973        373
    ##  3 Baltimore, MD     2827       1825
    ##  4 Baton Rouge, LA    424        196
    ##  5 Birmingham, AL     800        347
    ##  6 Boston, MA         614        310
    ##  7 Buffalo, NY        521        319
    ##  8 Charlotte, NC      687        206
    ##  9 Chicago, IL       5535       4073
    ## 10 Cincinnati, OH     694        309
    ## # ℹ 41 more rows

``` r
filter_balt =
  wash_homi_df |> 
  filter(city_state == "Baltimore, MD")

count_balt =
  filter_balt |> 
  summarize(
    total = n(),
    unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

test_balt =
  prop.test(
    x = count_balt$unsolved,
    n = count_balt$total
  ) |> 
  broom::tidy()

test_balt |> select(estimate, conf.low, conf.high) |> 
  knitr::kable(digits = 3)
```

| estimate | conf.low | conf.high |
|---------:|---------:|----------:|
|    0.646 |    0.628 |     0.663 |

Based on these estimates, we can conclude that about 64.6% of total
homicides committed in the city of Baltimore are not solved. Therefore,
we are 95% confident that the true proportion of unsolved homicides in
the city of Baltimore, however, lies between 62.8% and 66.3%

``` r
city_ptest =
  wash_homi_df |> 
  group_by(city_state) |> 
  summarize(
    total = n(),
    unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    .groups = "drop"
  ) |> 
  mutate(
    c_ptest = map2(unsolved, total, \(u, t) prop.test(u, t)),
    tidy_c_ptest = map(c_ptest, broom::tidy)
  ) |> 
  unnest(tidy_c_ptest) |> 
  select(
    city_state,
    estimate,
    conf.low,
    conf.high
  )
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `c_ptest = map2(unsolved, total, function(u, t) prop.test(u,
    ##   t))`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

``` r
city_ptest
```

    ## # A tibble: 51 × 4
    ##    city_state      estimate conf.low conf.high
    ##    <chr>              <dbl>    <dbl>     <dbl>
    ##  1 Albuquerque, NM    0.386    0.337     0.438
    ##  2 Atlanta, GA        0.383    0.353     0.415
    ##  3 Baltimore, MD      0.646    0.628     0.663
    ##  4 Baton Rouge, LA    0.462    0.414     0.511
    ##  5 Birmingham, AL     0.434    0.399     0.469
    ##  6 Boston, MA         0.505    0.465     0.545
    ##  7 Buffalo, NY        0.612    0.569     0.654
    ##  8 Charlotte, NC      0.300    0.266     0.336
    ##  9 Chicago, IL        0.736    0.724     0.747
    ## 10 Cincinnati, OH     0.445    0.408     0.483
    ## # ℹ 41 more rows

``` r
city_ptest |> 
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) |> 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_errorbar(
    aes(ymin = conf.low, ymax = conf.high)
  ) +
  geom_point(color = "blue", size = 2) +
  coord_flip() +
  labs(
    title = "Figure 4. Proportion of Unsolved Homicides by Cities in the United States",
    x = "Cities",
    y = "Estimates (95% CI)"
  )
```

<img src="p8105_hw5_smo2147_files/figure-gfm/figure 4-1.png" width="90%" />

Figure 4 shows the proportion of unsolved homicides sorted by cities
from lowest to highest, showing their estimated proportions along with
their 95 confidence interval range. From this figure, the city of
Chicago has the highest proportion of unsolved homicides at around 73%,
where the city of Tulsa either reports a proportion of 0% unsolved or
there’s no data on the proportion of crimes unsolved. Overall, the city
of Richmond reports one of the lowest proportion of unsolved homicides
among our sample population.
